{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiPS4RkAbbFvUmTI+VTnEf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/izzul112/ML_MLO-Submission_1/blob/main/mlop_submission1_tweet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Submission 1: Klasifikasi Tweet di Twitter**\n",
        "\n",
        "Nama: Habib Azizul Haq\n",
        "\n",
        "Username dicoding: kuroba_izzul\n",
        "\n",
        "Dizaman sekarang kita sering menghabiskan waktu di media sosial, baik sekedar melihat-lihat saja atau memantau idola kita di media sosialnya. Tak jarang saat kita sedang berselancar di media sosial kita menemukan tweet yang dirasa kurang sedap atau terlalu vulgar. Tentu kita akan merasa risih dan terganggu oleh hal tersebut.\n",
        "\n",
        "Untuk mengatasi masalah tersebut kita bisa membuat sebuah model machine learning untuk mengklasifikasikan sebuah tweet, apakah tweet itu bermuatan posotif atau justru negatif."
      ],
      "metadata": {
        "id": "9nHFSimMPqDO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tahap Persiapan**\n",
        "\n",
        "Di tahap ini kita akan menginsatall semua library yang di butuhkan serta membuat virtual environment."
      ],
      "metadata": {
        "id": "HRfdwmZNVDqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install condacolab\n"
      ],
      "metadata": {
        "id": "FUiGzD-EVIZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "AKC-nQenxAb1",
        "outputId": "c2826f66-aec7-451c-b7f9-1fa5762a0269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "‚è¨ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "üì¶ Installing...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-dfa229587fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install -q condacolab'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcondacolab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcondacolab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/condacolab.py\u001b[0m in \u001b[0;36minstall_mambaforge\u001b[0;34m(prefix, env, run_checks)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \"\"\"\n\u001b[1;32m    180\u001b[0m     \u001b[0minstaller_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr\"https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0minstall_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstaller_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_checks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_checks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/condacolab.py\u001b[0m in \u001b[0;36minstall_from_url\u001b[0;34m(installer_url, prefix, env, run_checks)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"üì¶ Installing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     task = run(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;34m\"bash\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstaller_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-bfp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTimeoutExpired\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[0;34m(self, input, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stdin_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                 \u001b[0mstdout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install -q condacolab\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cek versinya jika perlu"
      ],
      "metadata": {
        "id": "wU3cqjM5VPJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda --version"
      ],
      "metadata": {
        "id": "m7L1QxGBxvzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Buat conda virtual environment, kita berinama mlops-tfx-sub-1-tweet"
      ],
      "metadata": {
        "id": "OYOVCU7PVS_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create --name mlops-tfx-sub-1-tweet python==3.9.15"
      ],
      "metadata": {
        "id": "oYwXZKTZx0NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Aktifkan conda virtual environment-nya"
      ],
      "metadata": {
        "id": "jgVM-4r1Vh1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!conda activate mlops-tfx-sub-1-tweet"
      ],
      "metadata": {
        "id": "nkeN9m59x8Ex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Kita clone repository github ML_MLO-Submission_1, di dalamnya terdapat dataset yang akan kita kunakan nanti dan requirements yang akan kita gunakan."
      ],
      "metadata": {
        "id": "eaFBb_AJVmYW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/izzul112/ML_MLO-Submission_1.git"
      ],
      "metadata": {
        "id": "oWXxHZrlx-9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Selanjutnya kita install semua library yang dibutuhkan untuk menjalankan proyek ini. Dengan menjalankan perintah berikut."
      ],
      "metadata": {
        "id": "5mMn1EceXEa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jupyter scikit-learn tensorflow tfx==1.11.0 flask joblib"
      ],
      "metadata": {
        "id": "HzmSU6WnyCu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Setelah itu kita install juga requirements yang di butuhkan."
      ],
      "metadata": {
        "id": "bPaiI6NpXtH6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/ML_MLO-Submission_1/requirements.txt --user"
      ],
      "metadata": {
        "id": "YFNQcS95yFCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Terakhir kita import Tensorflow dan component dari TensorFlow Extended (TFX)."
      ],
      "metadata": {
        "id": "0hRz9GZkYbBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tfx.components import CsvExampleGen, StatisticsGen, SchemaGen, ExampleValidator, Transform, Trainer\n",
        "from tfx.proto import example_gen_pb2\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "import os"
      ],
      "metadata": {
        "id": "Vc0KIF0PyJEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Set Variabel**\n",
        "\n",
        "Disini kita akan mendefinisikan variabel yang akan kita gunakan nanti."
      ],
      "metadata": {
        "id": "hFj6w1sVZZrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PIPELINE_NAME = \"emailspam-pipeline\"\n",
        "SCHEMA_PIPELINE_NAME = \"emailspam-tfdv-schema\"\n",
        "\n",
        "#Directory untuk menyimpan artifact yang akan dihasilkan\n",
        "PIPELINE_ROOT = os.path.join('pipelines', PIPELINE_NAME)\n",
        "\n",
        "# Path to a SQLite DB file to use as an MLMD storage.\n",
        "METADATA_PATH = os.path.join('metadata', PIPELINE_NAME, 'metadata.db')\n",
        "\n",
        "# Output directory where created models from the pipeline will be exported.\n",
        "SERVING_MODEL_DIR = os.path.join('serving_model', PIPELINE_NAME)\n",
        "\n",
        "# from absl import logging\n",
        "# logging.set_verbosity(logging.INFO)"
      ],
      "metadata": {
        "id": "M9jBHKj5yLNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pastikan lokasi url dari variabel DATA_ROOT sudah benar dan didalamnya sudah ada datasetnya."
      ],
      "metadata": {
        "id": "04qNbd9oZnel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_ROOT = '/content/ML_MLO-Submission_1/data'"
      ],
      "metadata": {
        "id": "w3RtM9AJyQOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Tahapan Data Ingestion**\n",
        "\n",
        "Dalam tahap ini kita akan melakukan proses pengumpulan data dari berbagai sumber dan memastikan data tersebut memiliki format yang dapat diproses oleh machine learning pipeline. "
      ],
      "metadata": {
        "id": "SD9-7Xa2Z4SR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Kita bagi terlebih dahulu dataset kita menjadi 'train' untuk proses training dan 'eval' untuk proses evaluasi dengan perbandingan 8:2."
      ],
      "metadata": {
        "id": "Y8fyKl4GajDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = example_gen_pb2.Output(\n",
        "    split_config = example_gen_pb2.SplitConfig(splits=[\n",
        "        example_gen_pb2.SplitConfig.Split(name=\"train\", hash_buckets=8),\n",
        "        example_gen_pb2.SplitConfig.Split(name=\"eval\", hash_buckets=2)\n",
        "    ])\n",
        ")\n",
        "example_gen = CsvExampleGen(input_base=DATA_ROOT, output_config=output)"
      ],
      "metadata": {
        "id": "xsyJnbXmyS5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. setelah itu kita memanggil fungsi InteractiveContext() untuk menampilkan komponen TFX secara iteraktif, kita definisikan variabelnya interactive_context untuk memanggil fungsi InteractiveContext() dengan parameter pipeline_root yang kita isi juga dengan variabel PIPELINE_ROOT yang sudah kita definisikan di bagian Set Vaiabel."
      ],
      "metadata": {
        "id": "UiFeiFBlbcgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_context = InteractiveContext(pipeline_root=PIPELINE_ROOT)"
      ],
      "metadata": {
        "id": "ndry-CX6yWRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Setelah kita definisikan kita jalankan untuk melihat tampilan dari example_gen dengan kode dibawah."
      ],
      "metadata": {
        "id": "5q4LdxOIcmDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_context.run(example_gen)"
      ],
      "metadata": {
        "id": "Apb3YxfGyZlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Tahapan Data Validation**\n",
        "\n",
        "Dibagian ini kita akan melihat dan menganal dataset kita lebih dalam. Proses ini memiliki 3 tahap diantaranya:\n",
        "1. Membuat summary statistic\n",
        "2. Membuat data schema\n",
        "3. Mengidentifikasi anomali pada dataset"
      ],
      "metadata": {
        "id": "yyvUp0Kmx9i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Membuat summary statistic**\n",
        "\n",
        "Kita akan membuat summary statistic menggunakan komponen **StatisticsGen()**. Komponen memiliki parameter input berupa examples untuk menerima dataset dari komponen ExampleGen.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari statistics_gen dengan kode `interactive_context.run(statistics_gen)`."
      ],
      "metadata": {
        "id": "WYd_CVXXzIV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen.outputs[\"examples\"]\n",
        ")\n",
        " \n",
        " \n",
        "interactive_context.run(statistics_gen)"
      ],
      "metadata": {
        "id": "ymrr6nBn9Jqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk tampilan lebih menarik kita akan menggunakan **interactive_context.show()** untuk menampilkan summary statistic yang telah dibuat."
      ],
      "metadata": {
        "id": "PL49lj8J0k50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_context.show(statistics_gen.outputs[\"statistics\"])"
      ],
      "metadata": {
        "id": "CqQlrQur9NXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Membuat data schema**\n",
        "\n",
        "Setelah membuat summary statistic, kita akan membuat data schema. Kita menggunakan komponen **ShcemaGen()** untuk membuat data schema. Komponen ini akan menerima input berupa summary statistic.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari schema_gen dengan kode `interactive_context.run(schema_gen)`."
      ],
      "metadata": {
        "id": "nuzZYFQC062y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "schema_gen = SchemaGen(    statistics=statistics_gen.outputs[\"statistics\"]\n",
        ")\n",
        "interactive_context.run(schema_gen)"
      ],
      "metadata": {
        "id": "_JoeczcL-Vub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk tampilan lebih menarik kita akan menggunakan **interactive_context.show()** untuk menampilkan schema_gen yang telah dibuat."
      ],
      "metadata": {
        "id": "599pZvIG2lK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_context.show(schema_gen.outputs[\"schema\"])"
      ],
      "metadata": {
        "id": "8zCvK_h7-zeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dapat dilihat dataset kita diatas memiliki 3 fitur yaitu \"id\", \"label\" dan \"tweet\", tapi kita hanya akan memakai 2 fitur saja yaitu \"label\" dan \"tweet\", karena fitur \"id\" hanya berisi nomor urut. Selain itu, berdasarkan data schema di atas ketiga fitur tersebut haruslah lengkap untuk setiap dataset."
      ],
      "metadata": {
        "id": "BB9IKdVC20aw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Mengidentifikasi anomali pada dataset**\n",
        "\n",
        "Tahap selanjutnya adalah mengidentifikasi anomali pada dataset. Proses ini dilakukan menggunakan komponen **ExampleValidator()** yang telah disediakan oleh TFX. Komponen ini membutuhkan keluaran dari **statistics_gen** dan **schema_gen** sebagai parameter input.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari example_validator dengan kode `interactive_context.run(example_validator)`."
      ],
      "metadata": {
        "id": "bL96S-lg4Lq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_validator = ExampleValidator(\n",
        "    statistics=statistics_gen.outputs['statistics'],\n",
        "    schema=schema_gen.outputs['schema']\n",
        ")\n",
        "interactive_context.run(example_validator)"
      ],
      "metadata": {
        "id": "24nkPfFa-6ZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk tampilan lebih menarik kita akan menggunakan **interactive_context.show()** untuk menampilkan example_validator yang telah dibuat."
      ],
      "metadata": {
        "id": "TqxFICBF7yFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_context.show(example_validator.outputs['anomalies'])"
      ],
      "metadata": {
        "id": "y_PkqUWI-9zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Terlihat dari hasil diatas, dataset kita tidak memiliki aniomali."
      ],
      "metadata": {
        "id": "wLfoKjI7786k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Tahapan Data Preprocessing**\n",
        "\n",
        "Setelah melakukan data validation, tahap berikutnya adalah melakukan data preprocessing. Tujuan dari tahap ini adalah mengubah data mentah menjadi data yang siap digunakan untuk melatih model. Pada latihan ini, kita menggunakan TFT dan komponen Transform untuk melakukan data preprocessing.\n",
        "\n",
        "Ketika melakukan data preprocessing, Anda sangat disarankan untuk menempatkan preprocessing function ke dalam sebuah module tersendiri. Hal ini dilakukan karena komponen Transform akan menerima inputan berupa module file yang berisi preprocessing function. "
      ],
      "metadata": {
        "id": "FCyiSYQI8ix0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sebelum membuat module, kita perlu mendefinisikan nama dari module tersebut."
      ],
      "metadata": {
        "id": "qcVGxXS1AXrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRANSFORM_MODULE_FILE = \"tweet_transform.py\""
      ],
      "metadata": {
        "id": "JD_zeBYi_Bvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Selanjutnya, kita dapat membuat sebuah module dengan kode berikut.\n",
        "\n",
        "Module ini berisi dua buah function, yaitu `transformed_name()` dan `preprocessing_fn()`. \n",
        "\n",
        "1. Function `transformed_name()` digunakan untuk mengubah nama fitur yang telah di-transform. Perubahan ini dilakukan dengan menambahkan ‚Äú_xf‚Äù pada nama fitur tersebut.\n",
        "\n",
        "2. function `preprocessing_fn()`, kita hanya menerapkan tahapan preprocessing yang sederhana. Tahapan yang dilakukan hanya mengubah data yang terdapat dalam fitur \"tweet\" ke dalam bentuk lowercase. Selain itu, kita juga memastikan data pada fitur \"label\" memiliki format integer (tf.int64)."
      ],
      "metadata": {
        "id": "nMCsZCn-AehZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TRANSFORM_MODULE_FILE}\n",
        "import tensorflow as tf\n",
        "LABEL_KEY = \"label\"\n",
        "FEATURE_KEY = \"tweet\"\n",
        "def transformed_name(key):\n",
        "    \"\"\"Renaming transformed features\"\"\"\n",
        "    return key + \"_xf\"\n",
        "def preprocessing_fn(inputs):\n",
        "    \"\"\"\n",
        "    Preprocess input features into transformed features\n",
        "    \n",
        "    Args:\n",
        "        inputs: map from feature keys to raw features.\n",
        "    \n",
        "    Return:\n",
        "        outputs: map from feature keys to transformed features.    \n",
        "    \"\"\"\n",
        "    \n",
        "    outputs = {}\n",
        "    \n",
        "    outputs[transformed_name(FEATURE_KEY)] = tf.strings.lower(inputs[FEATURE_KEY])\n",
        "    \n",
        "    outputs[transformed_name(LABEL_KEY)] = tf.cast(inputs[LABEL_KEY], tf.int64)\n",
        "    \n",
        "    return outputs"
      ],
      "metadata": {
        "id": "F-HTesd9_HQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode di atas akan menghasilkan sebuah module dengan nama tweet_transform.py."
      ],
      "metadata": {
        "id": "gWl6agy-ApaH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahap selanjutnya adalah mendefinisikan komponen **Transform()**. Komponen ini akan menerima input berupa **examples** untuk menerima dataset dari komponen **ExampleGen**, **schema** untuk menerima data schema dari **SchemaGen**, dan **module_file** untuk menerima module file yang berisi **preprocessing function**.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari transform dengan kode `interactive_context.run(transform)`."
      ],
      "metadata": {
        "id": "HlAQ3nfNBc-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform  = Transform(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    schema= schema_gen.outputs['schema'],\n",
        "    module_file=os.path.abspath(TRANSFORM_MODULE_FILE)\n",
        ")\n",
        "interactive_context.run(transform)"
      ],
      "metadata": {
        "id": "0uJlj9TB_SNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Berdasarkan gambar di atas, dapat dilihat bahwa komponen **Transform** akan menghasilkan beberapa output mulai dari transform_graph, transformed_examples (transformed data), dll. Keseluruhan output nantinya digunakan untuk melakukan preprocessing terhadap serving set. Hal ini akan membantu kita untuk memastikan preprocessing yang dilakukan pada fase training sama dengan fase deployment sehingga dapat mencegah feature skew."
      ],
      "metadata": {
        "id": "Va_ZdroxCluB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Tahapan Pengembangan Model**\n",
        "\n",
        "Pada tahap ini, kita akan membuat dan melatih model menggunakan komponen Trainer. Komponen Trainer akan membutuhkan beberapa inputan salah satunya adalah sebuah module file yang berisi *training function*. Pada latihan ini, kita akan membuat sebuah module file berisi beberapa fungsi berikut."
      ],
      "metadata": {
        "id": "S_DuyZGjC2Oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Fungsi transformed_name()**\n",
        "\n",
        "Fungsi ini digunakan untuk mengubah nama fitur yang telah melalui proses transform."
      ],
      "metadata": {
        "id": "mZNp4Lvh8Sut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transformed_name(key):\n",
        "    \"\"\"Renaming transformed features\"\"\"\n",
        "    return key + \"_xf\""
      ],
      "metadata": {
        "id": "wVmjkdCJ_btZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Fungsi gzip_reader_fn()**\n",
        "\n",
        "Fungsi ini digunakan untuk memuat data dalam format TFRecord."
      ],
      "metadata": {
        "id": "JX97asIc8a33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gzip_reader_fn(filenames):\n",
        "    \"\"\"Loads compressed data\"\"\"\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')"
      ],
      "metadata": {
        "id": "r20VebQD_hF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Fungsi input_fn()**\n",
        "\n",
        "Fungsi ini digunakan untuk memuat transformed_feature yang dihasilkan oleh komponen **Transform** dan membaginya ke dalam beberapa batch."
      ],
      "metadata": {
        "id": "FBBg9whM8wtx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(file_pattern, \n",
        "             tf_transform_output,\n",
        "             num_epochs,\n",
        "             batch_size=64)->tf.data.Dataset:\n",
        "    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n",
        "    \n",
        "    # Get post_transform feature spec\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy())\n",
        "    \n",
        "    # create batches of data\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key = transformed_name(LABEL_KEY))\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "PyDMiFq3_jwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Fungsi model_builder()**\n",
        "\n",
        "Fungsi ini bertanggung jawab dalam membuat arsitektur model. Kita menggunakan salah satu *embedding layer* yang tersedia dan dapat diunduh melalui [TensorFlow Hub](https://tfhub.dev/google/universal-sentence-encoder/4)."
      ],
      "metadata": {
        "id": "8H6ZSCpA9Ajh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "VOCAB_SIZE = 10000\n",
        "SEQUENCE_LENGTH = 100\n",
        " \n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQUENCE_LENGTH)\n",
        " \n",
        "embedding_dim=16\n",
        "def model_builder():\n",
        "    \"\"\"Build machine learning model\"\"\"\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "    reshaped_narrative = tf.reshape(inputs, [-1])\n",
        "    x = vectorize_layer(reshaped_narrative)\n",
        "    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name=\"embedding\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "    \n",
        "    model.compile(\n",
        "        loss = 'binary_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    \n",
        "    )\n",
        "    \n",
        "    # print(model)\n",
        "    model.summary()\n",
        "    return model "
      ],
      "metadata": {
        "id": "AYPGECJx_qKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Fungsi _get_serve_tf_examples_fn()**\n",
        "\n",
        "Fungsi ini digunakan untuk menjalankan tahapan preprocessing data pada *raw request data*."
      ],
      "metadata": {
        "id": "RgXGMTeL94N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    \n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "    \n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        \n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        \n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "        \n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        \n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        \n",
        "        # get predictions using the transformed features\n",
        "        return model(transformed_features)\n",
        "        \n",
        "    return serve_tf_examples_fn"
      ],
      "metadata": {
        "id": "cNQ8z-QH_xus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Fungsi run_fn()**\n",
        "\n",
        "Fungsi ini bertanggung jawab untuk menjalankan proses training model sesuai dengan parameter training yang diberikan."
      ],
      "metadata": {
        "id": "7UbyXVdP-AKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        "\n",
        "def run_fn(fn_args: FnArgs) -> None:\n",
        "    \n",
        "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
        "    \n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir = log_dir, update_freq='batch'\n",
        "    )\n",
        "    \n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "    \n",
        "    \n",
        "    # Load the transform output\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    # Create batches of data\n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
        "    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
        "    vectorize_layer.adapt(\n",
        "        [j[0].numpy()[0] for j in [\n",
        "            i[0][transformed_name(FEATURE_KEY)]\n",
        "                for i in list(train_set)]])\n",
        "    \n",
        "    # Build the model\n",
        "    model = model_builder()\n",
        "    \n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(x = train_set,\n",
        "            validation_data = val_set,\n",
        "            callbacks = [tensorboard_callback, es, mc],\n",
        "            steps_per_epoch = 1000, \n",
        "            validation_steps= 1000,\n",
        "            epochs=10)\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
        "                                    tf.TensorSpec(\n",
        "                                    shape=[None],\n",
        "                                    dtype=tf.string,\n",
        "                                    name='examples'))\n",
        "    }\n",
        "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "metadata": {
        "id": "EJtp7vcW_3i1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nah, seluruh fungsi di atas, akan disatukan ke dalam sebuah module file. Namun, sebelum membuat module file, kita perlu mendefinisikan nama dari module tersebut."
      ],
      "metadata": {
        "id": "kk4RVhsm-QZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINER_MODULE_FILE = \"tweet_trainer.py\""
      ],
      "metadata": {
        "id": "PymFX0Mj_-KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jika sudah buat semua fungsi yang sudah kita tulis di atas ke dalam sebuah file modul seperi kode di bawah."
      ],
      "metadata": {
        "id": "9JZK-S0M-ZDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {TRAINER_MODULE_FILE}\n",
        "import tensorflow as tf\n",
        "import tensorflow_transform as tft \n",
        "from tensorflow.keras import layers\n",
        "import os  \n",
        "import tensorflow_hub as hub\n",
        "from tfx.components.trainer.fn_args_utils import FnArgs\n",
        " \n",
        "LABEL_KEY = \"label\"\n",
        "FEATURE_KEY = \"tweet\"\n",
        " \n",
        "def transformed_name(key):\n",
        "    \"\"\"Renaming transformed features\"\"\"\n",
        "    return key + \"_xf\"\n",
        " \n",
        "def gzip_reader_fn(filenames):\n",
        "    \"\"\"Loads compressed data\"\"\"\n",
        "    return tf.data.TFRecordDataset(filenames, compression_type='GZIP')\n",
        " \n",
        " \n",
        "def input_fn(file_pattern, \n",
        "             tf_transform_output,\n",
        "             num_epochs,\n",
        "             batch_size=64)->tf.data.Dataset:\n",
        "    \"\"\"Get post_tranform feature & create batches of data\"\"\"\n",
        "    \n",
        "    # Get post_transform feature spec\n",
        "    transform_feature_spec = (\n",
        "        tf_transform_output.transformed_feature_spec().copy())\n",
        "    \n",
        "    # create batches of data\n",
        "    dataset = tf.data.experimental.make_batched_features_dataset(\n",
        "        file_pattern=file_pattern,\n",
        "        batch_size=batch_size,\n",
        "        features=transform_feature_spec,\n",
        "        reader=gzip_reader_fn,\n",
        "        num_epochs=num_epochs,\n",
        "        label_key = transformed_name(LABEL_KEY))\n",
        "    return dataset\n",
        " \n",
        "# os.environ['TFHUB_CACHE_DIR'] = '/hub_chace'\n",
        "# embed = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        " \n",
        "# Vocabulary size and number of words in a sequence.\n",
        "VOCAB_SIZE = 10000\n",
        "SEQUENCE_LENGTH = 100\n",
        " \n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower_and_strip_punctuation\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQUENCE_LENGTH)\n",
        " \n",
        " \n",
        "embedding_dim=16\n",
        "def model_builder():\n",
        "    \"\"\"Build machine learning model\"\"\"\n",
        "    inputs = tf.keras.Input(shape=(1,), name=transformed_name(FEATURE_KEY), dtype=tf.string)\n",
        "    reshaped_narrative = tf.reshape(inputs, [-1])\n",
        "    x = vectorize_layer(reshaped_narrative)\n",
        "    x = layers.Embedding(VOCAB_SIZE, embedding_dim, name=\"embedding\")(x)\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "    x = layers.Dense(32, activation=\"relu\")(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "    \n",
        "    \n",
        "    model = tf.keras.Model(inputs=inputs, outputs = outputs)\n",
        "    \n",
        "    model.compile(\n",
        "        loss = 'binary_crossentropy',\n",
        "        optimizer=tf.keras.optimizers.Adam(0.01),\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
        "    \n",
        "    )\n",
        "    \n",
        "    # print(model)\n",
        "    model.summary()\n",
        "    return model \n",
        " \n",
        " \n",
        "def _get_serve_tf_examples_fn(model, tf_transform_output):\n",
        "    \n",
        "    model.tft_layer = tf_transform_output.transform_features_layer()\n",
        "    \n",
        "    @tf.function\n",
        "    def serve_tf_examples_fn(serialized_tf_examples):\n",
        "        \n",
        "        feature_spec = tf_transform_output.raw_feature_spec()\n",
        "        \n",
        "        feature_spec.pop(LABEL_KEY)\n",
        "        \n",
        "        parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "        \n",
        "        transformed_features = model.tft_layer(parsed_features)\n",
        "        \n",
        "        # get predictions using the transformed features\n",
        "        return model(transformed_features)\n",
        "        \n",
        "    return serve_tf_examples_fn\n",
        "    \n",
        "def run_fn(fn_args: FnArgs) -> None:\n",
        "    \n",
        "    log_dir = os.path.join(os.path.dirname(fn_args.serving_model_dir), 'logs')\n",
        "    \n",
        "    tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "        log_dir = log_dir, update_freq='batch'\n",
        "    )\n",
        "    \n",
        "    es = tf.keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', mode='max', verbose=1, patience=10)\n",
        "    mc = tf.keras.callbacks.ModelCheckpoint(fn_args.serving_model_dir, monitor='val_binary_accuracy', mode='max', verbose=1, save_best_only=True)\n",
        "    \n",
        "    \n",
        "    # Load the transform output\n",
        "    tf_transform_output = tft.TFTransformOutput(fn_args.transform_graph_path)\n",
        "    \n",
        "    # Create batches of data\n",
        "    train_set = input_fn(fn_args.train_files, tf_transform_output, 10)\n",
        "    val_set = input_fn(fn_args.eval_files, tf_transform_output, 10)\n",
        "    vectorize_layer.adapt(\n",
        "        [j[0].numpy()[0] for j in [\n",
        "            i[0][transformed_name(FEATURE_KEY)]\n",
        "                for i in list(train_set)]])\n",
        "    \n",
        "    # Build the model\n",
        "    model = model_builder()\n",
        "    \n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(x = train_set,\n",
        "            validation_data = val_set,\n",
        "            callbacks = [tensorboard_callback, es, mc],\n",
        "            steps_per_epoch = 1000, \n",
        "            validation_steps= 1000,\n",
        "            epochs=10)\n",
        "    signatures = {\n",
        "        'serving_default':\n",
        "        _get_serve_tf_examples_fn(model, tf_transform_output).get_concrete_function(\n",
        "                                    tf.TensorSpec(\n",
        "                                    shape=[None],\n",
        "                                    dtype=tf.string,\n",
        "                                    name='examples'))\n",
        "    }\n",
        "    model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ],
      "metadata": {
        "id": "2b3Rgv7dAHuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode di atas akan menghasilkan sebuah module dengan nama tweet_trainer.py. Module tersebut mengandung training function beserta beberapa helper function."
      ],
      "metadata": {
        "id": "qRmIbiCJ_Lur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tahap selanjutnya adalah mendefinisikan komponen **Trainer()**. Komponen ini akan menerima beberapa input seperti berikut. \n",
        "\n",
        "* **module_file** untuk menerima module file yang mengandung mengandung training \n",
        "function beserta beberapa helper function.\n",
        "* **examples** untuk menerima dataset dari komponen ExampleGen.\n",
        "* **schema** untuk menerima data schema dari komponen SchemaGen.\n",
        "* **tansform_graph** untuk menerima transform graph yang dihasilkan dari komponen Transform.\n",
        "* **train_args** untuk menampung parameter training.\n",
        "* **eval_args** untuk menampung parameter testing atau evaluation.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari trainer dengan kode `interactive_context.run(trainer)`."
      ],
      "metadata": {
        "id": "sX4wITGJ_wH0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.proto import trainer_pb2\n",
        " \n",
        "trainer  = Trainer(\n",
        "    module_file=os.path.abspath(TRAINER_MODULE_FILE),\n",
        "    examples = transform.outputs['transformed_examples'],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    train_args=trainer_pb2.TrainArgs(splits=['train']),\n",
        "    eval_args=trainer_pb2.EvalArgs(splits=['eval'])\n",
        ")\n",
        "interactive_context.run(trainer)"
      ],
      "metadata": {
        "id": "BZY7QOi6AZmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Membuat Tahapan Analisis dan Validasi Model**\n",
        "\n",
        "Apabila proses pengembangan model berjalan dengan lancar, tahap selanjutnya adalah membuat tahapan analisis dan validasi model. Untuk menjalankan tahapan ini kita perlu mendefinisikan dua buah komponen seperti berikut.\n",
        "\n",
        "1. **Komponen Resolver**\n",
        "2. **Komponen Evaluator**"
      ],
      "metadata": {
        "id": "Cbs0ueYqAY44"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Komponen Resolver**\n",
        "\n",
        "Untuk melakukan analisis dan validasi model, kita perlu menyediakan sebuah baseline model. Hal ini sangat penting terutama ketika kita memiliki lebih dari satu versi model dan ingin membandingkan dua buah versi model yang berbeda. Untuk melakukannya, kita bisa memanfaatkan komponen **Resolver()**.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari model_resolver dengan kode `interactive_context.run(model_resolver)`."
      ],
      "metadata": {
        "id": "N8FVOQpkA34i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.dsl.components.common.resolver import Resolver \n",
        "from tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy import LatestBlessedModelStrategy \n",
        "from tfx.types import Channel \n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing \n",
        " \n",
        "model_resolver = Resolver(\n",
        "    strategy_class= LatestBlessedModelStrategy,\n",
        "    model = Channel(type=Model),\n",
        "    model_blessing = Channel(type=ModelBlessing)\n",
        ").with_id('Latest_blessed_model_resolver')\n",
        " \n",
        "interactive_context.run(model_resolver)"
      ],
      "metadata": {
        "id": "bjgATXRqB--J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Komponen Evaluator**\n",
        "\n",
        "Setelah mendefinisikan komponen Resolver, tahap selanjutnya adalah membuat beberapa konfigurasi untuk mengevaluasi model. \n",
        "\n",
        "Kita akan mengatur beberapa konfigurasi seperti metrik untuk mengevaluasi model beserta nilai threshold dari suatu metrik. Selain menentukan metrik, kita juga dapat mengatur pembagian kelompok data berdasarkan fitur tertentu pada parameter slicing_specs\n",
        "\n",
        "Konfigurasi ini dibuat menggunakan library TFMA seperti berikut."
      ],
      "metadata": {
        "id": "1uQoWhnQBSQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_analysis as tfma \n",
        " \n",
        "eval_config = tfma.EvalConfig(\n",
        "    model_specs=[tfma.ModelSpec(label_key='label')],\n",
        "    slicing_specs=[tfma.SlicingSpec()],\n",
        "    metrics_specs=[\n",
        "        tfma.MetricsSpec(metrics=[\n",
        "            \n",
        "            tfma.MetricConfig(class_name='ExampleCount'),\n",
        "            tfma.MetricConfig(class_name='AUC'),\n",
        "            tfma.MetricConfig(class_name='FalsePositives'),\n",
        "            tfma.MetricConfig(class_name='TruePositives'),\n",
        "            tfma.MetricConfig(class_name='FalseNegatives'),\n",
        "            tfma.MetricConfig(class_name='TrueNegatives'),\n",
        "            tfma.MetricConfig(class_name='BinaryAccuracy',\n",
        "                threshold=tfma.MetricThreshold(\n",
        "                    value_threshold=tfma.GenericValueThreshold(\n",
        "                        lower_bound={'value':0.5}),\n",
        "                    change_threshold=tfma.GenericChangeThreshold(\n",
        "                        direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                        absolute={'value':0.0001})\n",
        "                    )\n",
        "            )\n",
        "        ])\n",
        "    ]\n",
        " \n",
        ")"
      ],
      "metadata": {
        "id": "p-BOWDWdCDBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setelah membuat konfigurasi yang akan digunakan untuk mengevaluasi model, tahap berikutnya adalah mendefinisikan komponen Evaluator. Komponen ini akan menerima beberapa input seperti berikut.\n",
        "\n",
        "* **examples** untuk menerima dataset dari komponen ExampleGen.\n",
        "* **model** untuk menerima model yang dihasilkan dari komponen Trainer.\n",
        "* **baseline_model** untuk menampung baseline model yang disediakan oleh komponen Resolver.\n",
        "* **eval_config** untuk menerima konfigurasi untuk mengevaluasi model.\n",
        "\n",
        "Setelah itu kita jalankan untuk melihat tampilan dari evaluator dengan kode `interactive_context.run(evaluator)`."
      ],
      "metadata": {
        "id": "Y3RNJsxPCgf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.components import Evaluator\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs['examples'],\n",
        "    model=trainer.outputs['model'],\n",
        "    baseline_model=model_resolver.outputs['model'],\n",
        "    eval_config=eval_config)\n",
        " \n",
        "interactive_context.run(evaluator)"
      ],
      "metadata": {
        "id": "z5gOkJjjCJD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hasil evaluasi dari komponen evaluator dapat kita visualisasikan menggunakan library TFMA seperti contoh kode di bawah ini."
      ],
      "metadata": {
        "id": "HCjXs0mGC98A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the evaluation results\n",
        "eval_result = evaluator.outputs['evaluation'].get()[0].uri\n",
        "tfma_result = tfma.load_eval_result(eval_result)\n",
        "tfma.view.render_slicing_metrics(tfma_result)\n",
        "tfma.addons.fairness.view.widget_view.render_fairness_indicator(\n",
        "    tfma_result\n",
        ")"
      ],
      "metadata": {
        "id": "cjaV_MWlDWgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Menambahkan Komponen Pusher**"
      ],
      "metadata": {
        "id": "5zqgsxLoDsIL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tfx.components import Pusher \n",
        "from tfx.proto import pusher_pb2 \n",
        " \n",
        "pusher = Pusher(\n",
        "model=trainer.outputs['model'],\n",
        "model_blessing=evaluator.outputs['blessing'],\n",
        "push_destination=pusher_pb2.PushDestination(\n",
        "    filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "        base_directory='serving_model_dir/sarchasm-detection-model'))\n",
        " \n",
        ")\n",
        " \n",
        "interactive_context.run(pusher)"
      ],
      "metadata": {
        "id": "7qZTZmtNDb8q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}